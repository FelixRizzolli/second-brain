<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
        SYSTEM "https://resources.jetbrains.com/writerside/1.0/xhtml-entities.dtd">
<topic xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="https://resources.jetbrains.com/writerside/1.0/topic.v2.xsd"
       title="SEO (Search Engine Optimization)" id="seo">
    <show-structure for="chapter,procedure" depth="2"/>

    <chapter title="Was ist SEO?" id="was-ist-seo">
        SEM (Search Engine Marketing) umfasst die Teilgebiete SEA (Search Engine Advertising - Suchmaschinenwerbung) und
        SEO (Search Engine Optimization – Suchmaschinenoptimierung).
    </chapter>

    <chapter title="Allgemeines" id="allgemeines">

        <chapter title="Suchmaschinen" id="allgemeines-suchmaschinen">
            <p>
                SEM (Search Engine Marketing) umfasst die Teilgebiete SEA (Search Engine Advertising -
                Suchmaschinenwerbung) und SEO (Search Engine Optimization – Suchmaschinenoptimierung).
            </p>
            <p>
                Anfangs war das Web noch relativ klein, und es wurden Listen von Webservern erstellt. Tim Berners-Lee
                pflegte selbst eine solche Liste. Es entwickelten sich Webkataloge, wie z.B. das Yahoo-Verzeichnis oder
                Open Directory Project (ODP) welche ein hierarchisches Verzeichnis von Websites in Kategorien und
                Unterkategorien erstellten.
            </p>
            <p>
                Da sich das WWW stark ausdehnte, musste eine Software her, welche das WWW permanent nach neuen Webseiten
                durchsucht und einen Index der gefundenen Seiten automatisch erstellt. Die erste Suchmaschine solcher
                Art war JumpStation (1993). Sie verwendete einen Webroboter welcher die Titel und Überschriften der
                Webseiten in einem Index speicherte und für Suchtreffer nutzte. Ein Jahr später erschien WebCrawler
                welcher den gesamten Text einer Webseite für den Aufbau des Suchindexes verwendete.
            </p>

            <chapter title="Google" id="allgemeines-google">
                <p>
                    Sergey Brin und Larry Page suchten eine Lösung für das Problem, dass die Qualität der Suchergebnisse
                    immer weiter abnahm. Webseitenbetreiber verwendeten verschiedene Tricks, um ihre Seite in den
                    Suchergebnissen auf die oberen Plätze zu bringen, auch wenn diese für die von den Suchenden
                    eingegebenen Suchbegriffe gar nicht relevant waren.
                </p>
                <p>
                    Ihre Lösung war: die Hypothese, dass die Verlinkung der Webseiten im WWW zu berücksichtigen, bessere
                    Resultate liefern. Sie entwickelten einen neuen Algorithmus: das sog. PageRank-Verfahren, womit es
                    Google tatsächlich gelang, deutlich bessere Suchergebnisse zu generieren als die Konkurrenz.
                </p>
            </chapter>

            <chapter title="Bing" id="allgemeines-bing">
                <p>
                    Die Suchmaschine von Microsoft hieß in ihren Anfängen MSN-Search, welche Großteils auf
                    Suchergebnisse anderer Suchmaschinen Zugriff. Sie entwickelten in den folgejahren eine eigene
                    Suchtechnologie und benannten ihre Suchmaschine in Windows Live Search, die ein Jahr später nur noch
                    Live Search und seit 2009 unter den Namen Bing bekannt ist. Alle Suchanfragen an die Yahoo-Websites
                    werden heute von Bing beantwortet.
                </p>
            </chapter>

            <chapter title="Yandex" id="allgemeines-yandex">
                <p>
                    Yandex ist mit 60 % (Stand 2019) Marktanteil in Russland die am meisten verwendete Suchmaschine
                    und ist durch eine Kooperation mit Microsoft als die voreingestellte Suchmaschine in Windows 10 für
                    den russischen Markt. Mittlerweile bietet Yandex ein ähnlich großes Angebot wie Google und könnte zu
                    einem ernstzunehmenden Konkurrenten heranwachsen.
                </p>
            </chapter>

            <chapter title="Baidu" id="allgemeines-baidu">
                <p>
                    Baidu ist mit knapp 60 % (Stand 2019) die marktführende Suchmaschine in China. Die arbeitet eng.
                    mit den chinesischen Behörden zusammen und blockiert Inhalte, welche von der Regierung zensiert
                    werden. Durch eine Kooperation mit Microsoft werden Suchanfragen mit englischen Begriffen werden an
                    Bing weitergeleitet und dafür ist Baidu die voreingestellte Suchmaschine für Windows 10 auf dem
                    chinesischen Markt.
                </p>
            </chapter>

            <chapter title="Alternative Suchmaschinen" id="allgemeines-alternativen">
                <p>
                    Heute gewinnen auch immer mehr neuere Suchmaschinen wie DuckDuckGo, Qwant, Ixquick und Startpage an
                    Marktanteilen, welche damit werben keine persönlichen Informationen zu sammeln.
                </p>
            </chapter>
        </chapter>
    </chapter>

    <chapter title="Aufbau und Funktionsweise" id="aufbau">
        <p>
            Heute (Stand 2019) nutzen mehr als 3,8 Milliarden Menschen das Internet (52 % der Weltbevölkerung). Wie
            viele Webseiten es insgesamt gibt, kann nicht gesagt werden. Analytiker schätzen den Google-Index bzw. das
            WWW heute auf 47 bis 50 Milliarden Webseiten.
        </p>

        <chapter title="Systemkomponenten einer Suchmaschine" id="aufbau-systemkomponenten">
            <chapter title="Das Webcrawler-System" id="aufbau-webcrawler">
                <p>
                    Das Webcrawler-System besteht aus Computerprogrammen, welche das WWW automatisch durchsuchen und
                    Webseiten herunterladen und analysieren können. Der Crawler startet auf einer beliebigen Webseite
                    und lädt diese für die weitere Analyse und Verarbeitung herunter.
                </p>

                <list>
                    <li>
                        <strong>Freshbots</strong> – besuchen neu gefundene Seiten. Sie analysieren vor allem reine
                        Textinhalte und gehen nicht sehr tief in eine Seite hinein. Wird eine Seite nicht regelmäßig
                        aktualisiert, so lässt auch die Besuchsfrequenz der Freshbots nach.
                    </li>
                    <li>
                        <strong>Deepbots</strong>  – berücksichtigen ein größeres Spektrum an Dateitypen (Bilder,
                        PDF ...) und erfassen möglichst viele Seiten einer Website.
                    </li>
                </list>
            </chapter>
            <chapter title="Der Scheduler" id="aufbau-scheduler">
                <p>
                    Der Scheduler sammelt und verwaltet die Adressen der Webseiten. Er bekommt von Crawlern ständig neue
                    URLs gemeldet, die diese in den besuchten Webseiten finden. Findet er in den Webseiten Verlinkungen
                    zu anderen Seiten, dann gleicht er diese mit den bereits gefundenen URLs ab, und sendet diese Seite
                    evtl. einem Crawler.
                </p>
                <p>
                    Da nahezu alle Webseiten direkt oder indirekt miteinander verlinkt sind, können Crawler somit das
                    ganze WWW durchwandern. Trotzdem gibt es immer noch Webseiten, zu denen kein Link führt. Auch jene
                    Seiten, die nur erreichbar sind, wenn man ein Formular ausfüllt oder sich registriert sind für
                    Crawler nicht erreichbar; diese Seiten gehören zum sog. DeepWeb.
                </p>
            </chapter>
            <chapter title="Der Index" id="aufbau-index">
                <p>
                    Der Index beinhaltet die Website in ihren Bestandteilen (Text, Bilder, Videos, HTML-Code) zerlegt
                    welche von den Crawlern heruntergeladen wurden. Bei der Suche durchsucht die Suchmaschine den Index
                    und nicht die Originalseite. Der Index ist quasi das Abbild des WWW und enthält dessen Seiten zu
                    einem bestimmten Zeitpunkt.
                </p>
            </chapter>
            <chapter title="Das Suchinterface" id="aufbau-suchinterface">
                <p>
                    Das Suchinterface ist im Wesentlichen ein einzelnes Formularfeld, in das der Suchende Suchwärter
                    eingeben kann. Google bietet heute neben der normalen Suche auch eine „Erweiterte Suche“ wodurch der
                    Suchende durch zahlreiche Optionen die Suche verfeinern kann.
                </p>
            </chapter>
        </chapter>
    </chapter>

    <chapter title="Das Ranking" id="ranking">
        <p>
            Nach eigenen Aussagen berücksichtigt Google bei der Relevanzberechnung heute mehr als 200 Rankingfaktoren
            „Signale“. Sie gelten als Betriebsgeheimnis und die Rankingalgorythmen sowie Signale ändern sich ständig.
            Dabei kann ihre Bedeutung variieren, neue hinzukommen und ältere wegfallen.
        </p>

        <chapter title="Die Relevanzberechnung" id="ranking-relevanzberechnung">
            <p>
                Ein offensichtlicher Ansatz ist dabei, im Suchindex nachzuschauen, auf welchen Webseiten das Suchwort
                überhaupt vorkommt. Folgende Punkte gelten für die Relevanzberechnung:
            </p>

            <list>
                <li>

                </li>
                <li>
                    <p>
                        <strong>Häufigkeit des Suchbegriffs</strong> – kommt der Suchbegriff häufiger vor, so wird
                        angenommen, dass das Dokument relevanter ist. Hervorhebungen – die Relevanz steigt durch
                        Hervorhebung des Suchworts mittels entsprechenden HTML-Tags
                    </p>

                    <list>
                        <li>
                            Dokumenttitel <code>&lt;title&gt;</code>
                        </li>
                        <li>
                            Überschriften <code>&lt;h1&gt;</code> bis <code>&lt;h6&gt;</code>
                        </li>
                        <li>
                            Hervorhebung im Fließtext <code>&lt;strong&gt;</code>, <code>&lt;b&gt;</code>... In
                            Auflistungen <code>&lt;li&gt;</code>
                        </li>
                        <li>
                            In Hyperlinks <code>&lt;a&gt;</code>
                        </li>
                    </list>
                </li>
                <li>
                    <strong>Position innerhalb des Dokuments</strong> – weiter oben im Fließtext = relevanter
                </li>
                <li>
                    <strong>Kombination</strong> – Besonders relevant wird ein Dokument, wenn diese Kriterien kombiniert
                    werden.
                </li>
                <li>
                    <strong>Innerhalb der URL</strong> – im Domain-, Verzeichnis- oder Dateinamen
                </li>
            </list>

            <p>
                Der Webentwickler kann im HTML-Quellcode die Keywords unsichtbar machen und häufig wiederholen. Dadurch
                ist es einfach Suchergebnisse zu manipulieren. Die beiden Google-Gründer Larry Page und Sergey Brin
                entwickelten ein Konzept, welches die Lösung für dieses Problem darstellte: Das PageRank-Verfahren.
            </p>
        </chapter>

        <chapter title="Das PageRank-Verfahren" id="ranking-pagerank">
            <p>
                Googles Hypothese war es: die Verlinkung der Webseiten im WWW für die Relevanzberechnung zu
                berücksichtigen liefert bessere Resultate als herkömmliche Suchmaschinen. So sollen Webseiten, auf die
                sehr viele andere Seiten verlinken, die eine hohe Zahl eingehender Links (sog. Backlinks) hat, besonders
                gut und nützlicher sein als Webseite, auf die selten verlinkt wird. Die Link Popularity die absolute
                Zahl der auf eine Webseite verweisenden Hyperlinks.
            </p>
            <p>
                Ein ähnliches Prinzip existierte bereits: der Certification Index – wie häufiger eine wissenschaftliche
                Publikation zitiert wurde als umso wichtiger galt sie.
            </p>

            <chapter title="Der Zufallssurfer" id="ranking-pagerank-zufallssurfer">
            </chapter>

            <chapter title="PageRank Ermitteln" id="ranking-pagerank-ermitteln">
                <p>
                    In der Google-Toolbar wurde früher eine Form des PagRank-Wertes angezeigt. Er wurde Toolbar-PageRank
                    genannt. Er basiert auf einer logarithmischen Skala und zeigte werte von 0 bis 10 an. Diese Funktion
                    wurde jedoch eingestellt.
                </p>
                <p>
                    Es gibt immer noch Drittanbieter, welche mit eigenen Methoden einem dem Google Toolbar PageRank
                    ähnlichen Wert ermitteln, um die Suchmaschinenoptimierung zu erleichtern.
                </p>
            </chapter>

            <chapter title="Weiterentwicklung des PageRank-Verfahrens" id="ranking-pagerank-weiterentwicklung">
                <p>
                    Das PageRank-Verfahren ist nicht zu 100 % robust gegen Manipulationen und es wurde viel Linkspam
                    betrieben, um einen besseren Platz auf der SERP zu erzielen. Es wurden Linkfarmen betrieben, deren
                    Zweck daran Bestand, durch hundertfache Verlinkungen von Websites und Webseiten untereinander den
                    PageRank einzelner Seiten zu erhöhen.
                </p>
                <p>
                    Um dem entgegenzuwirken, berücksichtigt Google nun weitere Kriterien und hat folgende Konzepte
                    entwickelt.
                </p>

                <list>
                    <li>
                        <p>
                            <strong>Die Distanz zwischen Webseiten</strong> – folgende Verlinkungen werden weniger stark
                            gewichtet
                        </p>

                        <list>
                            <li>Links innerhalb einer Webseite.</li>
                            <li>Verlinkte Websites auf demselben Server</li>
                            <li>Verlinkte Websites in naheliegender geografischer Region (ermittelt durch IP)</li>
                        </list>
                    </li>
                    <li>
                        <strong>Die Domain Popularity</strong> – ersetzt weitgehend das Konzept der Link Popularity.
                        Nicht mehr die absolute Zahl der eingehenden Links (Backlinks) ist entscheidend, sondern die
                        Zahl der eingehenden Links von unterschiedlichen Domains.

                    </li>
                    <li>
                        <strong>Die thematische Nähe</strong> – Mit der einführung des „Reasonable Surfer“ erfolgt die
                        Link-Auswahl des sog. „Zufallssurfers“ nicht mehr rein zufällig irgendeinen Link, sondern mit
                        höherer Wahrscheinlichkeit einem link, der einen thematischen Bezug zu der Webseite. Zudem
                        werden folgende Punkte beachtet: Hervorhebung eines Links (z.B. Links in Überschriften) und
                        Position des Links innerhalb des Dokuments (weiter oben = besser).

                    </li>
                    <li>
                        <strong>Der „intelligent Surfer“</strong> – Mit RankBrain ein KI-Algorythmus, in dem Google
                        schon seit Jahren investiert, wird die Suche des sog. „Zufallssurfers“ menschlicher. Mit
                        RankBrain werden gezielter Links aufgerufen welche einen Mehrwert darstellen und weiterführende
                        oder ergänzende Informationen zum Thema der Webseite haben.
                    </li>
                </list>

                <p>
                    Ohne qualitativ hochwertige Backlinks von Websites, die selbst über viel PageRank und Trust
                    verfügen, schafft es kaum eine Webseite auf die erste Ergebnisseite von Google und schon gar nicht
                    auf die Top-Rankingplätze.
                </p>
            </chapter>
        </chapter>

        <chapter title="Trust und TrustRank" id="ranking-trust">
            <p>
                Mit dem Trust-Wert hat Google einen neuen Wert in sein Rankingsystem eingeführt, welcher Auskunft
                gibt wie vertrauenswürdig die Informationen auf einer Website sind. <strong>TrustRank</strong> ist
                ein an das PageRank-Verfahren angelehnter Algorithmus zur Bewertung der Qualität von Webseiten.
            </p>
            <p>
                Besonders vertrauenswürdige Websites (<strong>Authority Domains</strong>) zeichnet aus, dass sie
                sorgfältig von Hand gepflegt und aktualisiert werden, dass sie mit einer sehr geringen
                Wahrscheinlichkeit auf schlechte Webseiten (z.B. Spamseiten) verlinken, jedoch mit hoher
                Wahrscheinlichkeit auf gute nützliche Sites.
            </p>
            <p>
                Seiten, die durch Linkkauf, Linktausch oder Linkfarmen einen unnatürlich hohen PageRank-Wert haben
                bekommen einen niedrigen TrustRank-Wert.
            </p>
        </chapter>

        <chapter title="Das BadRank-Konzept" id="ranking-badrank">
            <p>
                Mit dem BadRank hat Google einen neuen Wert eingeführt mit den Sites, welche gegen die
                Suchmaschinenrichtlinien verstoßen, indem sie Backlinks verkaufen oder Linkfarmen betreiben. Verweisen
                Links auf Seiten, die selbst einen hohen BadRank haben, erben die verlinkten Seiten von diesen BadRank.
                Websites, von denen man BadRank erben kann, bezeichnet Google als <strong>bad neighborhood</strong>.
            </p>
        </chapter>

        <chapter title="Lösung für Webspam" id="ranking-webspam">
            <p>
                Mit einem selbst lernenden Algorithmus im sog. <strong>Panda-Update</strong> versuchte Google gezielt
                minderwertige Seiten bzw. Domains zu identifizieren und in den Suchergebnissen herunterzustufen:
            </p>

            <list>
                <li>Eine geringe Menge an originalen Inhalten</li>
                <li>Hoher Prozentsatz an doppelten Inhalten</li>
                <li>Zu viel Werbung</li>
                <li>Seiteninhalt und Seitentitel stimmen nicht mit der Suchanfrage überein</li>
                <li>Unnatürlich häufiges Vorkommen eines Wortes auf einer Seite</li>
            </list>

            <p>
                Im sog. <strong>Penguin-Update</strong> wurden Websites, die über viele minderwertige Backlinks
                verfügten mit einem schlechteren Ranking bestraft. Davon waren stark Sites betroffen, die aggressiven
                und unvernünftigen Linkaufbau betrieben haben. Mit dem Update auf die Version 4 – Real Time Penguin
                wurde diese Abstrafung bzw. auch Erholung in Echtzeit eingeführt.
            </p>
            <p>
                Mit dem <strong>Hummingbird-Update</strong> wurde ein komplett neuer Ansatz eingeführt: Es sind nicht
                mehr unbedingt Seiten, die für die Keywords, die in einer Suche vorkommen, optimiert sind, sondern
                Seiten, die auf die Suchanfrage sie beste Antwort geben. Hierfür werden Textinhalte auf intelligente
                Art und Weise mithilfe von Algorithmen, die eine systematische Analyse vornehmen, untersucht.
            </p>
        </chapter>

        <chapter title="RankBrain" id="ranking-rankbrain">
            <p>
                15 % aller Sucheingaben sind für Google neu. Um Suchanfragen noch besser zu verstehen, die komplexer
                oder nicht eindeutig sind oder sie Google noch nie gesehen hat, wurde RankBrain eingeführt. RankBrain
                setzt zusätzlich die Technik des maschinellen Lernens (machine learning) ein. Es besitzt die Fähigkeit,
                aus jeder Analyse zu lernen und sich durch Erstellung neuer Verknüpfungen in seinem neuronalen Netz und
                damit verbundenen durch Erzeugung neuer Algorithmen selbstständig, d.h. ohne menschliches Zutun von
                Softwareentwicklern, weiterzuentwickeln. So wird RankBrain automatisch immer besser darin, Suchanfragen
                zu verstehen und die Webseiten mit den besten Rankings zu belohnen, die für die Nutzer den besten
                Nutzwert bieten.
            </p>
            <p>
                Ziel von Seitenbetreibern muss sein, mit hochwertigem einzigartigem und nützlichem Content sowie mit
                vorbildlicher Usability für das perfekte Nutzererlebnis zu sorgen.
            </p>
        </chapter>

        <chapter title="Die wichtigsten Punkte" id="ranking-wichtig">
            <table>
                <tr>
                    <td><strong>Content</strong></td>
                    <td>Qualitativ hochwertiger und einzigartiger Content.</td>
                </tr>
                <tr>
                    <td><strong>Backlinks</strong></td>
                    <td>Hochwertige Backlinks, die selbst hohe PageRank- und Trust-Werte haben.</td>
                </tr>
                <tr>
                    <td><strong>RankBrain</strong></td>
                    <td>Hoher Nutzwert.</td>
                </tr>
            </table>
        </chapter>
    </chapter>

    <chapter title="Die wichtigsten Signale" id="wichtigste-signale">

    </chapter>

    <chapter title="Der Prozess und die Ziele " id="prozess-ziele">

    </chapter>

    <chapter title="Keywords" id="keywords">

    </chapter>

    <chapter title="Content" id="content">

    </chapter>

    <chapter title="Webpage" id="webpage">

    </chapter>

    <chapter title="Site, Server &amp; Domain" id="site-server-domain">

    </chapter>

    <chapter title="Backlinks" id="backlinks">

    </chapter>

    <chapter title="Quellen" id="quellen">
        <tip>
            <strong>(BUCH)</strong>
            978-1791642211, SEO Praxisbuch, Thorsten Schneider, Independently published, 2018-12-13
        </tip>
    </chapter>
</topic>