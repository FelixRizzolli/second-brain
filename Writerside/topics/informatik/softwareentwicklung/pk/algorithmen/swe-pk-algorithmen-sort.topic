<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
        SYSTEM "https://resources.jetbrains.com/writerside/1.0/xhtml-entities.dtd">
<topic xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="https://resources.jetbrains.com/writerside/1.0/topic.v2.xsd"
       title="Sortieralgorithmen" id="swe-pk-algorithmen-sort">
    <show-structure for="chapter,procedure" depth="2"/>
    
    <chapter title="Einleitung" id="einleitung">
        <p>
            Ein Sortierverfahren ist ein Algorithmus, der dazu dient, Elemente einer Liste zu sortieren. Voraussetzung
            ist, dass zwischen den Elementen eine Ordnung möglich ist. Es müssen also Vorgänger- und
            Nachfolgerbeziehungen bzw. Gleichheit zwischen zwei zu vergleichenden Elementen bezüglich des
            Sortierkriteriums herstellbar sein. Diese Forderung wird als <emphasis><b>Trichotomie</b></emphasis>
            bezeichnet und sagt, dass für alle Elemente einer Menge bezüglich der Schlüssel <code>a</code>,
            <code>b</code> gilt:
        </p>

        <list>
            <li><code>a &lt; b</code> (lies: a vor b) oder</li>
            <li><code>a = b</code> oder</li>
            <li><code>a &gt; b</code> (lies: a nach b)</li>
        </list>

        <p>
            Um das mit Symbolen darzustellen, verwendet man die gezeigten Zeichen.
        </p>
        <p>
            Typische Beispiele sind die lexikografische Ordnung von Zeichenketten oder die numerische Ordnung von
            Zahlen.
        </p>
        <p>
            Eine weitere Voraussetzung dafür, dass man die Daten sortieren kann, ist
            <emphasis><b>Transitivität</b></emphasis>. Ist sie gegeben, dann ist gewährleistet, dass die Sortierung
            widerspruchsfrei durchgeführt werden kann. Es soll gelten: Gehört in der Liste Element <code>a</code> vor
            Element <code>b</code> und gehört Element <code>b</code> vor Element <code>c</code>, so folgt, dass Element
            <code>a</code> vor Element <code>c</code> anzuordnen ist.
        </p>
        <p>
            Wichtige Merkmale eines Sortier-Algorithmus sind die Zahl der durchschnittlich benötigten Schritte, bis die
            gewünschte Reihenfolge der Elemente vorliegt, und der benötigte <b>Speicherplatzbedarf</b>. Die Zahl der
            Schritte gibt die <b>Komplexität</b> des Algorithmus an und bestimmt maßgeblich, wie lange der gesamte
            Vorgang dauert.
        </p>
        <p>
            Sortierverfahren können in <b>interne</b> und <b>externe</b> Verfahren unterteilt werden. Wenn es möglich
            ist, die zu sortierenden Daten komplett im Hauptspeicher, zum Beispiel innerhalb einer Datenstruktur, zu
            sortieren, so liegt ein internes Sortierverfahren vor. Bei größeren Datenbeständen ist es nicht handhabbar,
            sämtliche Daten innerhalb des Arbeitsspeichers zu halten. Dann kommen externe Speichermedien wie Festplatten
            zum Einsatz. Die Effizienz des Algorithmus ist hier besonders wichtig, da die Lese- und Schreibzugriffe auf
            externe Medien viel Zeit beanspruchen.
        </p>
        <p>
            Unter <b>Effizienz</b> eines Suchverfahrens versteht man, wie viele Schritte im Durchschnitt notwendig sind,
            um den Sortiervorgang erfolgreich abzuschließen.
        </p>
        <p>
            Ein Sortier-Algorithmus wird als <b>stabil</b> bezeichnet, wenn er die relative Reihenfolge von Elementen
            beibehält, welche den gleichen Wert bezüglich des Sortierschlüssels aufweisen. Als <b>Sortierschlüssel</b>
            bezeichnet man das Element, nachdem die Sortierung erfolgt.
        </p>
    </chapter>

    <chapter title="Bubblesort" id="bubblesort">
        <chapter title="Grundlagen" id="bubblesort-grundlagen">
            <chapter title="Erklärung" id="bubblesort-grundlagen-erklaerung">
                <p>
                    Der Grundgedanke des Sortieralgorithmus kommt tatsächlich von Luftblasen. Ein Glas gefüllt mit
                    Mineralwasser: Darin steigen größere Luftblasen schneller auf als kleinere. Die sogenannten
                    "Bubbles" sortieren sich also der Größe nach. Deswegen kann das Sortierverfahren auch als
                    <b>"Sortieren durch Aufsteigen"</b> oder <b>"Austauschsortieren"</b> bezeichnet werden.
                </p>
                <p>
                    Der Bubble Sort gehört zu den Sortieralgorithmen mit einem vergleichsbasierten Verfahren. Dabei ist
                    das Sortierverfahren <b>stabil</b> und arbeitet <b>in-place</b>. Wegen seiner <b>durchschnittlichen
                    Zeitkomplexität von <code>O(n²)</code></b> gilt er als ziemlich langsam und wird deshalb in der
                    Praxis kaum verwendet.
                </p>
            </chapter>

            <chapter title="Prinzip" id="bubblesort-grundlagen-prinzip">
                <p>
                    Beim Bubblesort Algorithmus wird ein Array – also eine Eingabe-Liste – immer <b>paarweise von links
                    nach rechts in einer sogenannten Bubble-Phase durchlaufen</b>. Man startet also mit der ersten Zahl
                    und vergleicht diese dann mit ihrem direkten Nachbarn nach dem Sortierkriterium. Sollten beide
                    Elemente nicht in der richtigen Reihenfolge sein, werden sie ganz einfach miteinander vertauscht.
                    Danach wird direkt das nächste Paar miteinander verglichen, bis die gesamte Liste einmal durchlaufen
                    wurde. Die Phase wird so oft wiederholt, bis der gesamte Array vollständig sortiert ist.
                </p>
            </chapter>
        </chapter>

        <chapter title="Pseudocode" id="bubblesort-pseudocode">
            <code-block>
                laenge = x.Anzahl;
                for b = 1 to laenge
                    for k = 0 to (laenge-b)
                        if zahl[k] >  zahl[k+1]
                            c = zahl [k]
                            zahl[k] = zahl[k+1]
                            zahl[k+1] = c
            </code-block>
        </chapter>

        <chapter title="Bubblesort Laufzeit" id="bubblesort-laufzeit">
            <p>
                In der Praxis wird der Sortieralgorithmus kaum verwendet. Grund hierfür ist seine sehr lange Laufzeit,
                weswegen sich andere Sortierverfahren deutlich besser eignen. Beispielsweise der <b>Mergesort</b> oder
                der <b>Heapsort</b> sind bei einem Datensatz im über vierstelligem Bereich tausendmal schneller.
            </p>
            <p>
                Die Laufzeit im <b>Average-Case</b> beträgt genauso wie im <b>Worst-Case <code>O(n2)</code></b>. Das
                liegt daran, dass der Algorithmus paarweise voranschreitet und damit entsprechend viele Paare
                vergleichen muss.
            </p>
            <p>
                Nur im <b>Best-Case</b> kann er eine Laufzeit von <b><code>O(n)</code></b> erreichen. Das ist der Fall,
                wenn der Array bereits von Beginn an nach dem Sortierkriterium sortiert ist.
            </p>
            <p>
                Die Berechnung von der Anzahl benötigter Vergleiche einer Datenfolge von der Länge <code>n</code>, lässt 
                sich wie folgt darstellen:
            </p>
            
            <code-block lang="tex">
                n-1+n-2+...+1=\frac{(n(n-1)))}{2}\in O(n^{2})
            </code-block>
            
            <p>
                Bubblesort Laufzeit als Überblick:
            </p>
            
            <list>
                <li>Worst-Case: <code>O(n^2)</code></li>
                <li>Average-Case: <code>O(n^2)</code></li>
                <li>Best-Case: <code>O(n)</code></li>
            </list>
        </chapter>
    </chapter>

    <chapter title="Bucketsort" id="bucketsort">
        <chapter title="Grundlagen" id="bucketsort-grundlagen">
            <p>
                Der Bucketsort ist ein <b>nicht-vergleichsbasierter Sortieralgorithmus</b>. Er sortiert eine Liste von
                gleichmäßig verteilten Elementen sehr schnell in linearer Zeit. Das Verfahren erfolgt dabei in drei
                Schritten:
            </p>

            <list style="decimal">
                <li>
                    Zuerst erstellen wir sogenannte <b>Buckets</b>, in die wir dann die <b>Elemente</b> der unsortierten
                    Liste <b>verteilen und ablegen</b>.
                </li>
                <li>
                    Jeder einzelne dieser <b>Eimer</b> wird dann <b>mit einem weiteren Sortierverfahren</b> sortiert.
                    Diese können etwa der <b>Mergesort</b> oder der <b>Insertionsort</b>.
                </li>
                <li>
                    Der <b>Inhalt</b> der einzelnen Buckets wird dann durch eine Konkatenation <b>zu einer neuen
                    Gesamtliste zusammengefügt</b>.
                </li>
            </list>

            <p>
                Da der Sortieralgorithmus Zwischenspeicher verwendet, arbeitet das Sortierverfahren somit
                <b>out-of-place</b>.
            </p>
        </chapter>

        <chapter title="Algorithmus" id="bucketsort-algorithmus">
            <p>
                Der <b>Bucketsort Algorithmus</b> lässt sich also folgendermaßen darstellen:
            </p>

            <list style="bullet">
                <li>
                    Für den Sortiervorgang wird einerseits eine zu sortierende Liste benötigt, zusätzlich aber auch eine
                    Funktion, die jedes Element des Arrays einem Wert in dem Intervall <code>[0,n]</code> zuordnen kann.
                    Die Liste selbst setzt sich dabei aus <code>n</code>-Elementen zusammen.
                </li>
                <li>
                    Zum Sortieren werden Buckets benötigt, die das Intervall <code>[0,1]</code> in <code>n</code>
                    Teilintervalle mit der Größe <code>1/n</code> unterteilen kann. Entsprechend kann damit dann jedes
                    Element in den zugehörigen Eimer einsortiert werden. Für jeden Bucket wird dann für seine jeweiligen
                    Inhalte ein <b>Insertionsort</b> durchgeführt (Es kann auch ein <b>Mergesort</b> verwendet werden!).
                    Die Inhalte werden dann mittels Konkatenation zu einer fertig sortierten Gesamtliste zusammengefügt.
                </li>
            </list>

            <chapter title="Pseudocode" id="bucketsort-algorithmus-pseudocode">
                <code-block>
                    Bucketsort (liste, funktion)
                        n = liste.size
                        bucket = intervall(n)
                            for (element in liste)
                                bucket[floor (funktion(element) * n].add(element)
                        ausgabearray []
                        for (inhalt in buckets)
                            x.insertionsort(inhalt)
                            ausgabearray.append(x)
                        return ausgabearray
                </code-block>
            </chapter>
        </chapter>

        <chapter title="Komplexität" id="bucketsort-komplexitaet">
            <p>
                Die Komplexität hängt von mehreren Faktoren ab. Zum einen von der Verteilung der Funktionswerte. Dabei
                beträgt die <code>O</code>-Notation
            </p>

            <code-block lang="tex">
                O(n)+\sum_{n-1}^{i=0} O(l*log*l)
            </code-block>

            <p>
                Im günstigsten Fall sind die einzelnen Elemente annähernd <b>gleichverteilt</b>. Dann benötigt das
                Zuweisen in die Buckets, ebenso wie die Konkatenation eine <b>Gesamtlaufzeit</b> von <code>O(n)</code>.
                Bei anderen Werteverteilungen kann die Laufzeit jedoch vom Sortierverfahren, das für die einzelnen
                Listen verwendet wird, dominiert werden – also dem Insertionsort oder Mergesort. Die Komplexität wird
                also im Fall dieses Sortierverfahrens von verschiedenen Faktoren beeinflusst. <code>O(n)</code>
                entspricht dabei auch gleichzeitig dem Average-Case.
            </p>
            <p>
                Im Falle der Verwendung eines <b>Mergesorts</b> als angewendetes Sortierverfahren innerhalb eines
                Buckets, beträgt die Laufzeitkomplexität <code>O(n*log*n)</code>.
            </p>
            <p>
                Die <b>Speicherplatzkomplexität</b> ist <code>O(n)</code>.
            </p>
        </chapter>
    </chapter>

    <chapter title="Counting Sort" id="countingsort">
        <chapter title="Grundlagen" id="countingsort-grundlagen">
            <p>
                Die Sortiertechnik basiert dabei auf Schlüssel zwischen einem bestimmten Bereich. Dabei wird die
                <b>Anzahl der Elemente mit unterschiedlichen Schlüsselwerte gezählt</b>. Infolgedessen wird dann aus den
                Ergebnissen eine <b>sortierte Liste aufgebaut</b>.
            </p>

            <chapter title="Eigenschaften" id="countingsort-grundlagen-eigenschaften">
                <p>
                    Im Folgenden ein kurzer Überblick über die wichtigsten Eigenschaften eines Counting Sorts:
                </p>
                
                <list>
                    <li>
                        Das Verfahren gehört zu den <b>stabilen Sortieralgorithmen</b>.
                    </li>
                    <li>
                        Es handelt sich dabei um keinen vergleichsbasierter Sortieralgorithmus. Er arbeitet
                        <b>adressbasiert</b>.
                    </li>
                    <li>
                        Das Sortierverfahren zeigt sich am effizientesten, wenn der Bereich der Eingabedaten nicht
                        unmittelbar größer ist als die Anzahl der zu sortierenden Elemente. Entsprechend wird diese Form
                        der Sortierung meist <b>nur in einem mäßig großen Wertebereich verwendet</b>.
                    </li>
                    <li>
                        Die Counting Sort Laufzeitkomplexität kann allgemein mit <code>O(n)</code> inklusive einem
                        zusätzlichen Datenbereich als proportionalen Raum definiert werden.
                    </li>
                    <li>
                        Das Sortierverfahren kann <b>durch eine Erweiterung</b> auch für das <b>Sortieren von negativen
                        Zahlen</b> verwendet werden.
                    </li>
                </list>
            </chapter>
        </chapter>

        <chapter title="Funktionsweise" id="countingsort-funktionsweise">
            <p>
                Für den Algorithmus muss am Anfang eine zu sortierende <b>Eingabeliste</b> übergeben werden. Im ersten
                Schritt muss ein <b>Maximum der Zahlen</b> berechnet werden. Sollte es der Fall sein, dass es etwas
                Größeres als das Aktuelle gibt, soll dies nun das neue Größte sein. Im Anschluss soll <b>eine temporäre
                Liste in Form eines Hilfsarrays erstellt</b> werden. Im Anschluss werden dann <b>die Indizes aller
                Zahlen überprüft</b> und dadurch entsprechend <b>gezählt</b>, wie oft jede Zahl vorhanden ist. Die
                <b>Anzahl</b> wird entsprechend <b>in der temporären Liste gespeichert</b>. Jetzt geht es an die
                Sortierung, entsprechend wird die tatsächliche, <b>finale Position</b> der Zahl in der sortierten
                Reihenfolge gesucht. Beim Sortieren werden die Indizes der temporären Liste nach ihrer Anzahl geprüft.
                Dabei soll die Anzahl von <code>i</code> durchgegangen werden, um entsprechende <b>gleiche Elemente
                hintereinander einzutragen</b>. Die Zahl soll nun entsprechend in der richtigen Reihenfolge an die
                entsprechende Position eingefügt werden, um dann als <b>sortierte Liste zurückgegeben</b> zu werden.
            </p>

            <chapter title="Pseudocode" id="countingsort-funktionsweise-pseudocode">
                <code-block>
                    <![CDATA[
                    countingsort int[] zahlen
                        maximumZahlen = zahlen [0]
                        for i=1; i < zahlen.länge; i++
                            if zahlen[i] > maximumZahlen
                                maximumZahlen = zahlen [i]
                        temporäreListe = int [maximum+1]
                        for i = 0; i < zahlen.länge; i++
                            temporäreliste[zahlen[i]]++
                        finaleposition = 0;
                        for int i = 0; i <= maximum; i++
                            for int j = 0; j < temporäreliste[i]; j++
                                zahlen[finaleposition] = i
                                finaleposition++
                        return zahlen
                    ]]>
                </code-block>
            </chapter>
        </chapter>

        <chapter title="Laufzeit" id="countingsort-laufzeit">
            <p>
                Die Counting Sort Laufzeit ist immer <b>abhängig von der Anzahl der Elemente einer Liste <code>n</code>
                und der Größe des entsprechenden Zahlenintervalls <code>k</code></b>. Deshalb handelt es sich bei der
                Sortierung um einen linearen Zeitaufwand. Durch die vorhandene <code>for</code>-Schleife besitzt dabei
                jeweils <code>n</code>- oder <code>k</code>-Durchläufe. Dadurch lässt sich eine Laufzeitkomplexität von
                <code>O(n+k)</code> bestimmen. Entsprechend kann es sich für den Sortieralgorithmus vorteilhaft
                auswirken, wenn die Intervalllänge im Gegensatz zur Anzahl der Objekte <code>n</code> deutlich kleiner
                ist.
            </p>

            <chapter title="Speicherkapazität" id="countingsort-laufzeit-speicherkapazitaet">
                Dadurch, dass der Counting Sort out-of-place arbeitet, braucht der Algorithmus eine temporäre Liste/ein
                Hilfsarray zur Zwischenspeicherung unserer Zahlenwerte. Entsprechend beträgt der Speicherplatz
                <code>k</code>-Elemente, wodurch die Speicherplatzkomplexität <code>O(n+k)</code> entspricht.
            </chapter>
        </chapter>
    </chapter>

    <chapter title="Heapsort" id="heapsort">
        <chapter title="Grundlagen" id="heapsort-grundlagen">
            <p>
                Der Heapsort wurde von Robert W. Floyd und J. W. J Williams entwickelt. Er gehört zu den <b>instabilen
                Sortieralgorithmen</b>, arbeitet dabei aber nach dem <b>in-place</b>-Prinzip. Die Funktionsweise basiert
                eigentlich hauptsächlich auf <b>binären Heap Eigenschaften</b> als zentrale Datenstruktur und ist dabei
                ein <b>fast vollständiger binärer Baum</b>. Die Knoten enthalten dann die Daten, die letztendlich
                sortieren werden sollen.
            </p>
            <p>
                Das <b>Sortierverfahren</b> hat eine Zeitkomplexität von <code>O(n log(n))</code>, damit gibt es keinen
                asymptotisch schnelleren <b>Sortieralgorithmus</b> der vergleichsbasiert ist. Man kann ihn im
                Allgemeinen durch seine Vorgehensweise auch als Verbesserung zum <b>Selectionsort</b> verstehen.
            </p>
        </chapter>

        <chapter title="Funktionsweise" id="heapsort-funktionsweise">
            <p>
                Als erstes benötigt es eine Liste, die nach dem Heap Sort geordnet werden soll, in diesem Fall nach dem
                Max-Heap – also von klein nach groß. Im nächsten Schritt muss also aus dem Array ein max-Heap gebaut
                werden (<b>Build-Max-Heap</b>). Dann kommt auch schon der Hauptteil, welcher unterschiedlich benannt
                werden kann. Bekannt als: <b>Heapyfy, Downheap, Versickern</b> oder eventuell auch <b>Versenken</b>.
                Diese Prozedur beschreibt eine Prüfung der Heap-Bedingung bezüglich der Kinder mit ihrem jeweiligen
                Vater. Wenn die Heap-Bedingung nicht erfüllt ist, muss das größere Kind mit dem Vater ausgetauscht
                werden. Dies wird so oft wiederholt, bis die Heap-Size 1 ist, also nur noch ein "unsortiertes" Element
                besteht.
            </p>

            <chapter title="Pseudocode" id="heapsort-funktionsweise-pseudocode">
                <code-block>
                    <![CDATA[
                    Heapsort(arr) {
                        BuildMaxHeap(arr)
                        for i
                            tausche arr[1]  arr[i]
                            heapsize
                            Versickern(arr, 1)
                    }

                    BuildMaxHeap(arr) {
                        heapsize
                        for i
                            Versickern(arr, i)
                    }

                    Versickern(arr, i) {
                        l
                        r
                        if (l<=heapsize) and (arr[l]>arr[i])
                            largest
                        else
                            largest
                        if (r<=heapsize) and (arr[r]>arr[largest])
                            largest
                        if (largest != i) {
                            tausche arr[i]  arr[largest]
                            Versickern(arr, largest)
                        }
                    }
                    ]]>
                </code-block>
            </chapter>
        </chapter>

        <chapter title="Komplexität" id="heapsort-komplexitaet">
            <p>
                Die Heapsort Komplexität beträgt im Allgemeinen <code>O(nlog(n))</code> Vergleiche. Damit ist er
                bezüglich vergleichsbasierten-Sortieralgorithmen das schnellste asymptotische Sortierverfahren. Jedoch
                kann er trotzdem beispielsweise mit dem <b>Quicksort</b> in seiner standardisierten Form nicht
                mithalten. Da der Heapsort in <b>verschiedenen Prozeduren</b> abläuft, kann für jedes <b>eine eigene
                Laufzeitkomplexität</b> festgestellt werden. Der <b>downheap</b> benötigt beispielsweise
                <b><code>log(n)</code></b> Schritte beim Vergleichen. Der <b>buildheap</b> zeigt bei einer genaueren
                Analyse auf, dass er nur <code>O(n)</code> Vergleiche braucht.
            </p>
            <p>
                Wenn man sich die potenziellen Fälle genauer ansieht, kann gesagt werden, dass die Heapsort Laufzeit
                sowohl im <b>Worst Case</b>, als auch im <b>Average Case</b> und <b>Best Case</b>
                <code>O(nlog(n))</code> beträgt. Grund dafür ist, dass der Heap-Aufbau eine schrittweise vollständige
                Invertierung der Sortierreihenfolge benötigt.
            </p>
        </chapter>

        <chapter title="Varianten" id="heapsort-varianten">
            <p>
                Für den Heapsort gibt es <b>verschiedene Varianten</b>. Neben dem Standard-Heapsort gibt es noch den
                <b>Bottom-Up-Heapsort</b>, den <b>Smoothsort</b>, die <b>Ternären Heaps</b> und die <b>n-äre Heaps</b>,
                welche die Komplexität in verschiedenen Bereichen potenziell steigern können.
            </p>

            <chapter title="Bottom-Up-Heapsort" id="heapsort-varianten-bottomupheapsort">
                <p>
                    Die dabei populärste Variante ist der Bottom-Up-Heapsort, da er oft nur die Hälfte an
                    Vergleichsoperationen benötigt und kommt damit der Laufzeit des Quicksort sehr Nahe. Aber wie
                    funktioniert das? Da ein Binärbaum größtenteils aus Blättern besteht und sich durch das Sortieren
                    die niedrigeren Werte bereits abgesenkt werden mussten, arbeitet die Variante mit der Annahme, dass
                    ein Element bis zur Blattebene oder in der Nähe versickert werden muss. Entsprechend wird auf
                    Verdacht bis zur Blattebene abgesenkt und auf den zweiten Vergleich verzichtet. Der
                    Bottom-Up-Heapsort sollte aber nicht bei kleineren Feldern mit einfacher numerischer
                    Vergleichsoperation benutzt werden oder sehr viele Elemente gleichwertig sind. Heißt also, man
                    sollte ihn vor allem bei großen Datenmengen mit hohem Aufwand pro Vergleichsoperationen verwenden.
                </p>
            </chapter>

            <chapter title="Smoothsort" id="heapsort-varianten-smoothsort">
                <p>
                    Der Smoothsort ändert die Reihenfolge der Vorgehensweise. Während normalerweise vorsortierte Felder
                    keinen Vorteil erbringen, da immer das größte Element an die Wurzel wandern muss, um die Zahl
                    anschließend wieder mit dem letzten Knoten austauscht, arbeitet der Smoothsort direkt umgekehrt.
                    Jedoch ist bei der Umsetzung die Gewährleistung des Heapstatus beim Sortieren sehr aufwändig.
                </p>
            </chapter>

            <chapter title="Ternäre Heaps" id="heapsort-varianten-ternaereheaps">
                <p>
                    Wie man sich vielleicht schon denken kann, werden hier statt binären Heaps, ternäre Heaps benutzt.
                    Entsprechend läuft er nicht auf Basis eines Binärbaums, sondern ein vollständig besetzter Knoten hat
                    jeweils 3 Kinder. Dadurch können die Vergleichsoperationen reduziert werden. Beispielsweise bei sehr
                    großen Feldern, die mehrere Millionen Elemente enthalten, kann dabei 20-30 % der Vergleiche
                    eingespart werden, wenn man sich das gleiche aber bei kleineren Feldern ansieht, kann es sogar sein,
                    dass der Sortieralgorithmus dadurch langsamer als der Standard-Heapsort ist.
                </p>
            </chapter>

            <chapter title="n-äre Heaps" id="heapsort-varianten-naereheaps">
                <p>
                    Wie schon bei den Ternären Heaps, kann der Baum natürlich noch breiter, bzw. die Heaps noch flacher
                    gestaltet werden, durch das <b>Erweitern der Anzahl von Kindern eines Vaters</b>. Dadurch steigt die
                    Anzahl der Vergleichsoperationen zwar weiter an, jedoch können dadurch andere Vorteile bezüglich der
                    Effizienz von Caches geschaffen werden, da der sonstige Aufwand weiter gesenkt werden kann und
                    geordneter auf die Elemente zugegriffen werden kann.
                </p>
            </chapter>
        </chapter>
    </chapter>

    <chapter title="Insertionsort" id="insertionsort">
        <chapter title="Grundlagen" id="insertionsort-grundlagen">
            <p>
                Der Insertion Sort gehört zu den stabilen <b>Sortieralgorithmen</b> und kann als <b>Sortieren durch
                Einfügen</b> beschrieben werden, deswegen auch <b>Einfügesortierenmethode</b> genannt. Das Ganze lässt
                sich einfach durch die englischen Wörter insertion = Einfügen und sort = sortieren ableiten, weswegen
                der Sortieralgorithmus auch manchmal als <b>Insertsort</b> bezeichnet wird. Allgemein kann auch noch
                gesagt werden, dass der Sortieralgorithmus einfach zu implementieren ist und dabei bei kleiner <b>oder
                schon teilweise vorsortierten Eingabemengen sehr effizient</b> arbeitet. Da das Sortierverfahren keinen
                zusätzlichen Speicherplatz benötigt, arbeitet der Algorithmus <b>in-place</b>, was natürlich für seine
                Speicherplatzkomplexität spricht.
            </p>

            <chapter title="Prinzip" id="insertionsort-grundlagen-prinzip">
                <p>
                    Der <b>Grundgedanke</b> ist eigentlich ganz einfach, da es wahrscheinlich dem am nächsten kommt, wie
                    man die Zahlen selbst ordnen würde. Dabei stellt man sich ein Kartenspiel vor, welches der Größe
                    nach sortiert werden soll. In deiner Hand liegt die Karte Pik 5 und Pik 9. Als Nächstes wird die Pik
                    6 gezogen. Diese Karte wird in der Mitte zwischen den beiden anderen eingeordnet. Genau so geht der
                    Insertionsort auch vor.
                </p>
                <p>
                    Der Insertionsort durchläuft Schritt für Schritt einen Array und <b>entnimmt</b> dabei aus der
                    <b>unsortierten Eingabefolge ein Element</b> und <b>setzt</b> es dann an der entsprechend
                    <b>richtigen Stelle wieder ein</b> – „Sortieren durch Einfügen“. Die <b>restlichen Elemente</b> des
                    Arrays müssen dann wiederum <b>hinter dem neu eingefügten Wert verschoben</b> werden.
                </p>
            </chapter>
        </chapter>

        <chapter title="Algorithmus" id="insertionsort-algorithmus">
            <p>
                Zuerst wird eine Liste übergeben. Gestartet wird immer mit dem ersten Element, also stellt dieser den
                sortierten Bereich der Liste dar und der restliche Array gilt als unsortierter Bereich. Die Bedingung
                ist, dass so lange der unsortierte Bereich Elemente hat, soll immer das erste Element aus dem
                unsortierten Bereich an die richtige Stelle im sortierten Bereich eingeordnet werden. Zum Schluss erhält
                man dann eine sortierte Liste.
            </p>

            <chapter title="Pseudocode" id="insertionsort-algorithmus-pseudocode">
                <p>
                    Gegeben sei also eine Liste l. Als nächstes wird der sortierte Bereich mit dem ersten Element
                    <code>a</code> definiert. Das erste Element im unsortierten Bereich wird als <code>b</code>
                    bezeichnet. Das aktuelle <code>b</code> wird im Zwischenspeicher tmp gespeichert, solange der
                    unsortierte Bereich Elemente besitzt. Das Element wird aus dem unsortierten Bereich entfernt und
                    schließlich vom Zwischenspeicher in den sortierten Bereich eingefügt.
                </p>

                <code-block>
                    <![CDATA[
                    Insertionsort (Liste l)
                        for a in 0 -> l.lenght – 2
                            do
                                b = a + 1
                                tmp = l[b]
                            while b > 0 AND tmp < l[b-1] do

                            l[b] = l[b-1]
                            b – –
                            l[b] = tmp
                    ]]>
                </code-block>
            </chapter>
        </chapter>

        <chapter title="Laufzeit" id="insertionsort-laufzeit">
            <p>
                Die Insertion Sort Laufzeit ist abhängig von der Anzahl von benötigten Verschiebungen, also auch
                abhängig von der Anordnung von Elementen im unsortierten Bereich. Sowohl der durchschnittliche
                Average-Case als auch der Worst-Case haben die Komplexität <code>O(n2)</code>. Der schlechteste Fall
                tritt dann ein, wenn der Algorithmus beispielsweise absteigend sortiert ist und er dann aufsteigend
                sortiert werden soll. Im Fall, dass die Liste bereits von Beginn an sortiert ist, ist die Komplexität
                linear <code>O(n)</code>, dabei handelt es sich um den Best-Case. Dabei ist der Sortieralgorithmus sogar
                besser als der <b>Quicksort</b>, <b>Mergesort</b>, <b>Heapsort</b>, usw.
            </p>
            <p>
                Die <b>Insertion Sort Laufzeit</b> nochmal als <b>Überblick</b>:
            </p>

            <list>
                <li>
                    <p>
                        Worst-Case/ Schlechtester Fall:
                    </p>

                    <code-block lang="tex">
                        \frac{n(n-1)}{2}\in O(n^{2})
                    </code-block>
                </li>
                <li>
                    <p>
                        Average Case/ Durchschnittliche Anzahl von Vergleichen:
                    </p>

                    <code-block lang="tex">
                        \frac{n(n-1)}{4}\in O(n^{2})
                    </code-block>
                </li>
                <li>
                    <p>
                        Best-Case/ Bestes Szenario:
                    </p>

                    <code-block lang="tex">
                        O(n)
                    </code-block>
                </li>
            </list>

            <p>
                Wenn für das Sortierverfahren die binäre Suche verwendet wird, kann man die Anzahl der Vergleiche und
                Verschiebungen durch Sortieroperationen im Worst-Case folgendermaßen abschätzen:
            </p>

            <code-block lang="tex">
                \log(n!)\in O(n\log n-n\log e+\log n)=O(n\log n-0,4426n+\log n)
            </code-block>

            <chapter title="Komplexität" id="insertionsort-laufzeit-komplexität">
                <p>
                    Im Falle, dass der Array bereits "fast sortiert" ist, also jedes der Elemente fast am richtigen
                    Platz ist, kann die Laufzeit auch <code>O(n)</code> sein. In diesem Fall und auch im
                    Best-Case-Szenario ist der Insertion Sort ziemlich schnell. Jedoch in den anderen Fällen sehr
                    langsam weswegen sich das Sortierverfahren <b>nur für kleinere Datenmengen oder für das Einfügen von
                    weiteren Elementen</b> in eine schon geordnete Liste eignet.
                </p>
                <p>
                    Zusätzlich kann noch gesagt werden, dass der Sortieralgorithmus bezüglich Speicherplatzkomplexität
                    punkten kann. Da der Algorithmus mittel <b>in-place</b>-Verfahren arbeitet, wird kein zusätzlicher
                    Speicherplatz benötigt.
                </p>
            </chapter>
        </chapter>
    </chapter>

    <chapter title="Mergesort" id="mergesort">
        <chapter title="Allgemeines" id="mergesort-allgemeines">
            <p>
                Der Mergesort gehört zu den stabilen <b>Sortieralgorithmen</b>. Er leitet sich im Allgemeinen vom
                englischen "merge", also verschmelzen und "sort", dem sortieren ab. Der Sinn dahinter ist einfach nur,
                dass der Algorithmus die vorhandenen Daten als eine gesamte Liste betrachtet, die er dann in kleinere
                Listen unterteilt. Man kann also sagen, er zerlegt ein Gesamtproblem in mehrere Teilprobleme und löst
                diese dann Stück für Stück. Im Endeffekt setzt er sie dann zum Schluss zu einer Gesamtlösung zusammen.
                Heißt also verallgemeinert, dass der Algorithmus nach dem Grundsatz teile- und herrsche arbeitet. Das
                <b>Teile-und-herrsche-Verfahren</b> (eng. <emphasis>divide and conquer</emphasis>) stellt in der
                Informatik ein Paradigma für den Entwurf eines effizienten Algorithmus dar.
            </p>
        </chapter>

        <chapter title="Funktionsweise" id="mergesort-funktionsweise">
            <p>
                Die <b>Funktionsweise</b> lässt sich ganz einfach <b>in drei Schritten</b> erklären:
            </p>
            
            <list>
                <li>
                    Eine Liste wird in zwei Hälften zerlegt. Diese <b>Unterteilung</b> wird solange fortgesetzt, bis nur
                    noch ein Element in einer Menge vorhanden ist.
                </li>
                <li>
                    Dann werden alle Teilstücke für sich <b>sortiert</b>.
                </li>
                <li>
                    Anschließend müssen die Hälften dann <b>nach dem Suchkriterium zu einer Menge vermischt</b> werden.
                </li>
            </list>
        </chapter>

        <chapter title="Pseudocode" id="mergesort-pseudocode">
            <p>
                Der Algorithmus lässt sich in zwei Funktionen beschreiben, dafür wird zuerst einmal die Liste
                <code>a</code> eingegeben und in eine linke und rechte Hälfte halbiert. Für beide Seiten soll dann
                jeweils die Methode <code>merge_sort</code> ausgeführt werden (solange die Listen größer gleich 1 sind)
                und die verschmolzene sortierte Liste mit der Funktion verschmelzen zurückgegeben werden.
            </p>

            <code-block>
                <![CDATA[
                merge_sort (Liste a)
                    Falls die Liste a <= 1 ist, soll die Liste antworten – sonst
                        soll die Liste in linke Liste l und rechte Liste r halbiert werden.
                        l = merge_sort(l)
                        r = merge_sort(r)
                        zurückgeben verschmelze (l,r)
                ]]>
            </code-block>

            <code-block>
                <![CDATA[
                merge_sort (Liste a)
                    if a <= 1
                    do
                        int mitte = a.lenght / 2
                        int l -> i <= mitte – 1
                        int r -> i >= array.length – mitte – 1
                        l = merge_sort(l)
                        r = merge_sort(r)
                        return verschmelze(l,r)
                ]]>
            </code-block>
        </chapter>
    </chapter>

    <chapter title="Quicksort" id="quicksort">
        <chapter title="Definition" id="quicksort-definition">
            <p>
                Das Sortierverfahren gehört zu den <b>rekursiven und nicht stabilen Sortieralgorithmen</b>. Er lässt
                sich aus dem englischen quick = schnell und sort = sortieren ableiten und wurde in den sechziger Jahren
                von C. Antony R. Hoare in seiner Grundform entwickelt. Der Quicksort Algorithmus arbeitet wie der
                Mergesort nach dem <b>Teile-und-herrsche-Verfahren</b> (eng. <emphasis>divide and conquer</emphasis>)
                der Informatik.
            </p>

            <chapter title="Pivotelement" id="quicksort-definition-pivotelement">
                <p>
                    Das Pivotelement leitet sich vom französischem pivot = Dreh-/Angelpunkt ab. Es handelt sich dabei
                    immer um ein <b>Element einer Zahlenmenge</b>, welches als Erstes von einem Algorithmus <b>bestimmt
                    wird, um eine bestimmte Berechnung durchzuführen</b>. Innerhalb des Sortierverfahrens stellt das
                    Element sozusagen eine Aufteilungsgrenze dar. Dabei wird dann jeweils nach dem kleinsten oder
                    betragsmäßig größten Element in der aktuellen (Teil-)Liste gesucht und rekursiv sortiert. Die
                    Auswahl des Elements wird <b>Pivotisierung</b> genannt.
                </p>
            </chapter>
        </chapter>

        <chapter title="Erklärung" id="erklaerung">
            <p>
                Das genaue Prinzip hinter dem Quicksort kann man nicht unbedingt verallgemeinern. Es kann gewisse
                Abweichungen durch die jeweils verwendete Programmiersprache geben, wodurch der Ablauf tatsächlich
                unterschiedlich beeinflusst werden kann. Trotzdem gibt es ein gewisses Grundprinzip.
            </p>

            <chapter title="Pivotelement bestimmen" id="quicksort-erklaerung-pivotelement">
                <p>
                    Dafür kann man eigentlich alle Elemente verwenden. Heißt, dass sowohl das erste als auch das letzte
                    Element, einen Wert aus der Mitte oder sogar einen Zufallswert ausgewählt können. Für eine optimale
                    Rekursion verwendet man eigentlich immer den <b>Median</b>.
                </p>
            </chapter>

            <chapter title="Teile und herrsche" id="quicksort-erklaerung-teileundherrsche">
                <p>
                    Das Pivot-Element wird dann <b>in die Mitte gesetzt und die restlichen Werte sortiert</b>. Einmal
                    nach <b>links</b>, wenn sie <b>kleiner</b> sind und einmal nach <b>rechts</b>, wenn sie
                    <b>größer</b> sind. Dabei muss man die Elemente <b>immer der ursprünglichen Reihenfolge nach</b> von
                    links nach rechts in ihrem Bereich einordnen.
                </p>
            </chapter>

            <chapter title="Rekursiver Quicksort-Aufruf" id="quicksort-erklaerung-rekursiver">
                <p>
                    <b>Rekursiver Quicksort-Aufruf für beide Teile des Arrays (Vor und nach dem Pivot-Element):</b>
                </p>
                <p>
                    Das pivot-Element ist danach an seinem richtigen Platz und es müssen Neue bestimmt werden. Natürlich
                    wieder die ersten Elemente, aber <b>diesmal in beiden Bereichen</b>. Der Vorgang wird wiederholt,
                    somit werden die restlichen Elemente wieder genau im selben Schema neben den pivot-Elementen
                    eingeordnet. Im nächsten Schritt haben alle neuen pivot-Elemente keine Vergleichswerte mehr und sind
                    damit auch direkt richtig platziert und damit sind alle Werte sortiert.
                </p>
            </chapter>
        </chapter>

        <chapter title="Pseudocode für das In-Place-Verfahren" id="quicksort-pseudocode">
            <code-block>
                <![CDATA[
                funktion quicksort(li, re)
                    if li < re
                        t = teilen(li, re)
                        quicksort(li, t)
                        quicksort(t+1, re)
                    end
                end

                funktion teilen(li, re)
                    i = li – 1
                    j = re + 1
                    pivot = Liste[(li + re) / 2]
                    while Liste[i] < pivot
                        i++
                    while Liste [j] > pivot
                        j++
                    if (i < j)
                        int a = Liste[i]
                        Liste[i] = Liste[j]
                        Liste[j] = a
                    else return j
                ]]>
            </code-block>
        </chapter>
    </chapter>

    <chapter title="Radix Sort" id="radixsort">
        <chapter title="Grundlagen" id="radixsort-grundlagen">
            <p>
                Der Radixsort kann aus dem lateinischen radix = Wurzel; Basis abgeleitet werden. Er ist auch als
                Distributionsort oder Fachverteilen bekannt. Dabei handelt es sich um ein lineares Sortierverfahren
                basierend auf dem <b>Counting Sort</b> und <b>Bucketsort</b>. Es kann <b>entweder stabile out-of-place
                oder instabile in-place Varianten</b> realisiert werden. Des Weiteren kann zwischen Verfahren
                unterschiedenen werden. Zum einen das MSD-Verfahren (englisch most significant digit), bei dem die
                Stelle mit dem höchsten Wert beginnt oder das LSD-Verfahren (englisch least significant digit), bei dem
                die niedrigste Stelle beginnt und sich zur höchstwertigen Stelle vorarbeitet.
            </p>
            <p>
                Die Grundidee dahinter ist eigentlich nur, dass bei diesem Sortieralgorithmus mit Queues – also
                Warteschlagen – gearbeitet wird. Das bedeutet, dass mit jedem zusätzlichen Schritt eine weitere
                Sortierinformation resultiert. Das führt im Endeffekt zur fertigen Sortierung. Beim Radixsort bestehen
                die Schlüssel aus Zeichen eines endlichen Alphabets der zu ordnenden Elemente. Dabei muss eine totale
                Quasiordnung zwischen den Zeichen des Alphabets vorhanden sein.
            </p>
        </chapter>

        <chapter title="Vorgehensweise" id="radixsort-vorgehensweise">
            <p>
                Als Prinzip heißt das, dass in jedem Durchlauf die einzelnen Elemente durchgegangen und dabei nur einen
                bestimmten Teil davon begutachtet werden. Als Beispiel kann man sich ein Geburtsdatum vorstellen,
                welches aus Tag, Monat und Jahr besteht. Dabei startet man immer beim letzten Element (z.B. das Jahr)
                und arbeitet sich – Durchlauf für Durchlauf- zum ersten Element (z.B. Tag) in zwei Phasen vor.
            </p>

            <chapter title="Partitionierungsphase" id="radixsort-vorgehensweise-partitionierungsphase">
                <p>
                    In der Partitionierungsphase werden die <b>Daten in das vorhandene Fach aufgeteilt</b>. Dabei gibt
                    es für <b>jedes einzelne Zeichen</b> des jeweiligen Alphabets ein <b>eigenes Fach</b>. Für
                    Buchstaben also beispielweise jeweils ein Fach a, ein Fach b, etc. Man ordnet immer die
                    zusammengehörige Elementreihe entsprechend des geprüften Elements in eine passende Spalte ein. Heißt
                    also, wenn man das dritte Element der Reihe von hinten betrachtet, würde "abb" in das Fach "b"
                    einsortiert werden.
                </p>
            </chapter>

            <chapter title="Sammelphase" id="radixsort-vorgehensweise-sammelphase">
                <p>
                    Die zweite Phase ist die Sammelphase. Dabei fügt man die <b>Inhalte der einzelnen Fächer wieder zu
                    einer neu sortierten Liste zusammen</b>, beginnend mit dem Fach mit der niedrigsten Wertigkeit.
                    Dabei ist es wichtig zu beachten, dass die Reihenfolge der Elemente innerhalb der Fächer nicht
                    verändert werden darf.
                </p>
                <p>
                    Das Ganze wird <b>so oft wiederholt, bis alle einzelnen Stellen der Schlüssel einmal geprüft worden
                    sind</b>.
                </p>
            </chapter>

            <chapter title="Laufzeit" id="radixsort-vorgehensweise-laufzeit">
                <p>
                    <b>Die Laufzeit lässt sich in etwa mit <code>O(l*n)</code> abschätzen</b>. Das <b>l</b> ist dabei
                    die <b>Länge eines Schlüssels</b>. <b>n</b> stellt die <b>Anzahl der zu sortierenden Elemente</b>
                    dar. Entsprechend ist natürlich klar, dass die Laufzeit linear proportional mit der Anzahl der
                    vorhandenen Elemente ist – also n. Eine lineare Laufzeit ist davon abhängig, ob die maximale Länge
                    der zu sortierenden Elemente von Beginn an bekannt ist.
                </p>
            </chapter>
        </chapter>

        <chapter title="Beispiel" id="radixsort-beispiel">
            <code-block lang="java">
                <![CDATA[
                class Radix_Sortieralgorithmus {
                    public static void main (String[] args) {
                        int liste[] = {…};
                        int i = liste.length;
                        radixsort(liste, i);
                        ausgeben(liste, i);
                    }

                    static int maximum(int liste[], int i) {
                        int max = liste[0];
                        for (int a = 1; a < i; a++)
                            if (liste[a] > max)
                                max = liste[a];
                        return max;
                    }


                    static void countingsort(int liste[], int i, int faktor) {
                        int ausgabe[] = new int[i];
                        int a;
                        int zaehlen[] = new int[10];
                        Arrays.fill(zaehlen,0);

                        for (a = 0; a < i; a++)
                            zaehlen[ (liste[a]/faktor)%10 ]++;
                        for (a = 1; a < 10; a++)
                            zaehlen[a] += zaehlen[a – 1];

                        for (a = i – 1; a >= 0; a–) {
                            ausgabe[zaehlen[ (liste[a]/faktor)%10 ] – 1] = liste[a];
                            zaehlen[ (liste[a]/faktor)%10 ]–;
                        }

                        for (a = 0; a < i; a++)
                            liste[a] = ausgabe[a];
                    }

                    static void radixsort(int liste[], int i) {
                        int m = maximum(liste, i);

                        for (int faktor = 1; m/faktor > 0; faktor *= 10)
                            countingsort(liste, i, faktor);
                    }

                    static void ausgeben(int liste[], int i) {
                        for (int a=0; a
                            System.out.print(liste[a]+“ „);
                    }
                }
                ]]>
            </code-block>

            <chapter title="Iterativ" id="iterativ">
                <p>
                    Im Folgenden ist ein Radix Sort Java-Methode zum Sortieren einer Integer-Liste. Bei dem Code werden
                    potenzielle Vorzeichen nicht beachtet und sollte nur bei positiven Zahlen genutzt werden. Zusätzlich
                    wird eine Schleife über alle Bits der Schlüssel bei 32 Bit gesetzt.
                </p>

                <code-block lang="java">
                    <![CDATA[
                    public static void radixsort(int[] i) {
                        int nummer;
                        int[] anzahlfach = new int[2];
                        int[][] fach  = new int[2][i.length];
                        for (int j=0; j<32; j++) {
                            anzahlfach[0] = 0;
                            anzahlfach[1] = 0;
                            for (int k=0; k
                                nummer = (i[k]>>j)&1;
                                fach[nummer][anzahlfach[nummer]++] = i[k];
                            }
                            System.arraycopy(fach[0], 0, i, 0,       anzahlfach[0]);
                            System.arraycopy(fach[1], 0, i, anzahlfach[0], anzahlfach[1]);
                        }
                    }
                    ]]>
                </code-block>
            </chapter>
        </chapter>
    </chapter>
    
    <chapter title="Selectionsort" id="selectionsort">
        <chapter title="Grundlagen" id="grundlagen">
            <p>
                Das Sortierverfahren gehört in der Informatik zu den einfachen und <b>instabilen Sortieralgorithmen</b>.
                Er kann dir auch durch die Begriffe Selectsort oder Exchange Sort bekannt sein.
            </p>
            <p>
                Das allgemeine Prinzip kann man sich als <b>"Sortieren durch Auswahl"</b> merken. Und das geht in genau
                zwei Richtungen. Entweder sucht man dabei immer das kleinste (MinSort) oder das größte Element
                (MaxSort). Das Vorgehen dabei bleibt immer gleich.
            </p>

            <chapter title="Optimized Selection Sort Algorithm" id="selectionsort-grundlagen-ossa">
                <p>
                    Der <b>"Optimized Selection Sort Algorithm"</b> – kurz OSSA ist ein alternativer Ansatz des
                    Selectionsorts. Dabei werden beide Optionen der Sortierung gleichzeitig angewendet, heißt also, dass
                    der <b>MinSort und Maxsort parallel, gemeinsam arbeiten</b>. Entsprechend werden dabei dann immer
                    das größte und kleinste Element in der unsortierten Liste gesucht und entsprechend nach vorne oder
                    nach hinten eingesetzt werden. Dadurch kann im Normalfall eine Beschleunigung erreicht werden.
                </p>
            </chapter>
        </chapter>

        <chapter title="Funktionsweise und Pseudocode" id="selectionsort-funktionsweise">
            <p>
                Zuerst braucht es eine Liste. Diese ist anfangs komplett unsortiert, weswegen der sortierte Bereich zu
                Beginn als leer gilt. Die Bedingung ist dabei, dass solange der unsortierte Bereich noch ein Element
                enthält, soll nach dem kleinsten oder dem größten Element im unsortierten Bereich gesucht werden. Sollte
                ein Element des unsortierten Bereichs kleiner als der Vergleichswert sein, dann wird das passende
                Element aus dem unsortierten Bereich entfernt und in den Sortierten Bereich an seine richtige Position
                eingefügt. Am Ende erhalten wir dann eine sortierte Liste.
            </p>

            <code-block>
                Selectionsort (Liste l)
                    For i = 0 to n-2
                    Do
                        Minimum = i
                        For j = i + 1 to n-1
                        Do
                            If l[Minimum] > l[j] then Minimum = j
                                tmp = l[i]
                                l[i] = l[Minimum]
                                l[Minimum] = tmp
            </code-block>
        </chapter>

        <chapter title="Laufzeit" id="selectionsort-laufzeit">
            <p>
                Der Sortieralgorithmus ist ein ziemlich besonderer Fall, denn es sind wie beim <b>Mergesort</b> keine
                Unterschiede festzustellen. Die Laufzeit im <b>Selection Sort Worst Case entspricht genau der
                Komplexität im Best Case</b>. Damit beträgt die <b>Selection Sort Laufzeit immer <code>O(n2)</code></b>.
                Das hat einen ganz einfachen Grund, denn die Liste wird unabhängig von vorsortierten Daten immer von
                vorne bis hinten komplett durchlaufen. Deshalb benötigt er Vergleiche, wovon die allgemeine Selection
                Sort Laufzeit abgeleitet werden kann.
            </p>
        </chapter>
    </chapter>

    <chapter title="Shellsort" id="shellsort">
        <chapter title="Grundlagen" id="shellsort-grundlagen">
            <p>
                Der Shellsort, den Donald Shell ursprünglich entwickelte, ist Teil von den <b>instabilen
                Sortieralgorithmen</b>. Der Das Sortierverfahren arbeitet <b>in-place</b>. Der Shellsort weist
                prinzipiell das Verhalten eines <b>Insertionsort</b> auf, wodurch er eine Art Variante des
                Insertionsorts darstellt. Innerhalb des Sortieralgorithmus werden immer <b>Elemente verglichen, die
                einen bestimmten Abstand voneinander haben</b> und dabei nicht direkt benachbart sind. Wie groß die
                Feldbreite dabei sein muss, kann selbst bestimmt werden, sollte dabei aber möglichst effizient gewählt
                werden. Wenn ein Element weit bewegt werden muss, werden viele Bewegungen benötigt. Die Grundidee ist
                dabei, die Liste so umzusortieren, dass man durch das Entnehmen jedes h-Elements eine geordnete Folge
                erhält. Während der Sortierungen wird der Wert solange reduziert, bis er 1 wird, womit die Lücke
                zwischen den Elementen schrittweise verringert wird.
            </p>
        </chapter>

        <chapter title="Implementierung" id="shellsort-implementierung">
            <p>
                Aufgebaut auf dem eben verwendeten Beispiel, werden im Folgenden zwei mögliche Implementierungen
                aufgelistet. Dabei wird ein <b>Shellsort Java-Quellcode</b> und eine <b>Python-Implementierung</b>
                dargestellt.
            </p>

            <code-block lang="java" collapsed-title="JAVA" collapsible="true">
                <![CDATA[
                public class Shellsort{
                    public static void sort(int[] liste) {
                        int inner, outer;
                        int tmp;
                        int h = 1;
                        while (h <= liste.length / 4) {
                            h = h * 4+ 1;
                        }
                        while (h > 0) {
                            for (outer = h; outer < liste.length; outer++) {
                                tmp = liste[outer];
                                inner = outer;
                                while (inner > h – 1 && liste[inner – h] >= tmp) {
                                    liste[inner] = liste[inner – h];
                                    inner -= h;
                                }
                                liste[inner] = tmp;
                            }
                            h = (h – 1) / 4;
                        }
                    }

                    public static void main(String[] args) {
                        int [] liste= {4,10,2,8,1,7,12,3,6,11,5,9};
                        System.out.println(„Eingabe Liste: “ + Arrays.toString(liste));
                        sort(liste);
                        System.out.println(„Sortierte Liste:  “ + Arrays.toString(liste));
                    }
                }
                ]]>
            </code-block>

            <code-block lang="python" collapsed-title="PYTHON" collapsible="true">
                <![CDATA[
                def shellsort(liste):
                    h = len(liste)
                    gap = h//4
                    whilegap > 0:
                        for outer in range(gap,h):
                            tmp =liste[outer]
                            inner =outer
                            while  j >=gap andliste[inner-gap] >tmp:
                                liste[inner] =liste[inner-gap]
                                inner -=gap
                                liste[inner] =tmp
                            gap //=4
                        liste =[ 4, 10, 2, 8, 1, 7, 12, 3, 6, 11, 5, 9]
                        h =len(liste)
                    print(„Eingabeliste:“)
                    forouter inrange(h):
                        print(liste [outer]),

                shellsort(liste)
                print(„\nSortierte Liste:“)
                forouter inrange(h):
                    print(liste [outer]),
                ]]>
            </code-block>
        </chapter>

        <chapter title="Komplexität" id="shellsort-komplexitaet">
            <p>
                Warum sollte man den Shell Sort verwenden, wenn man doch gleich den Insertionsort implementieren könnte?
                Der Grund liegt darin, dass man durch die Aufteilung in Untersequenzen für viel <b>kürzere Strecken beim
                Sortierten</b> sorgen kann. Beim herkömmlichen Insertionsort kann der Weg zwischen den Elementen beim
                Austausch ziemlich groß sein, wodurch der Insertionsort in der Praxis zu einem ineffizienten
                Sortierverfahren werden kann.
            </p>
            <p>
                Beim Shellsort ist die Laufzeit – also die Komplexität – immer <b>abhängig von der Wahl der Folge für
                die Spaltenanzahl</b>. Dabei eine optimale Wahl zu treffen ist sehr schwierig. <b>Je größer die Wahl des
                Abstands</b> zwischen den einzelnen Sequenzen ist, <b>desto größer sind die entsprechenden
                Verschiebungen</b>. Im Gegensatz dazu sind bei der Wahl von <b>kleinen Abständen</b> die benötigte
                Anzahl von Durchläufen des Sortierverfahrens deutlich höher.
            </p>
            <p>
                Im Allgemeinen wurde aber experimentell belegt, dass im <b>Durchschnitt die Komplexität in etwa bei</b>
                <code>O(n^1.25)</code> liegt.
            </p>
            <p>
                Die Laufzeitkomplexität lässt sich jedoch durch die <b>Abhängigkeit bezüglich der Folgen</b>
                unterschiedlich darstellen, dafür werden im Folgenden <b>einige Beispiele</b> dargestellt:
            </p>

            <list>
                <li>
                    Die ursprüngliche Folge von Donald Shell 1, 2, 4, 8, 16,...,2^k hat eine Komplexität von
                    <code>O(n^2)</code>.
                </li>
                <li>
                    Die Folge von Hibbard 1, 3, 7, 15, 31,..., 2^k– 1 hat eine Laufzeit von <code>O(n^1.5)</code>.
                </li>
                <li>
                    Bei der Folge von Pratt 1, 2, 3, 4, 6, 8, 9, 12,..., 2^p3^q beträgt die Laufzeitkomplexität
                    <code>O(n*log(n)^2)</code>.
                </li>
                <li>
                    Eine weitere Folge ist von Sedgewick 1, 8, 23, 77, 281, 1073,...,4^(k+1)+3*2^k+1 zeichnet sich durch
                    eine Laufzeit von <code>O(n^(4/3))</code> aus.
                </li>
                <li>
                    Die Folge von Knuth 1, 4, 13, 40, 121, 364,...,(3^k-1)/2 hat wiederum eine Laufzeit von
                    <code>O(n^1.5)</code>.
                </li>
            </list>
        </chapter>
    </chapter>

    <chapter title="Quellen" id="quellen">
        <tip>
            <b>(BUCH)</b>
            978-3836244763, "Handbuch für Softwareentwickler", Veikko Krypczyk &amp; Elena Bochkor, 2018
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/bubblesort-1325" ignore-vars="true">
                https://studyflix.de/informatik/bubblesort-1325
            </a>, 2020-12-18 14:40
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/bucketsort-1438" ignore-vars="true">
                https://studyflix.de/informatik/bucketsort-1438
            </a>, 2020-12-18 15:50
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/counting-sort-1407" ignore-vars="true">
                https://studyflix.de/informatik/counting-sort-1407
            </a>, 2020-12-18 16:00
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/heapsort-1326" ignore-vars="true">
                https://studyflix.de/informatik/heapsort-1326
            </a>, 2020-12-18 16:10
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/insertionsort-1321" ignore-vars="true">
                https://studyflix.de/informatik/insertionsort-1321
            </a>, 2020-12-18 14:45
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/mergesort-1324" ignore-vars="true">
                https://studyflix.de/informatik/mergesort-1324
            </a>, 2020-12-18 14:45
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/quicksort-1322" ignore-vars="true">
                https://studyflix.de/informatik/quicksort-1322
            </a>, 2020-12-21 09:30
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/radix-sort-1408" ignore-vars="true">
                https://studyflix.de/informatik/radix-sort-1408
            </a>, 2020-12-18 14:45
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/selectionsort-1323" ignore-vars="true">
                https://studyflix.de/informatik/selectionsort-1323
            </a>, 2020-12-18 14:45
        </tip>
        <tip>
            <b>(WEBSEITE)</b>
            <a href="https://studyflix.de/informatik/shellsort-1411" ignore-vars="true">
                https://studyflix.de/informatik/shellsort-1411
            </a>, 2020-12-21 10:35
        </tip>
    </chapter>
</topic>